{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eaa775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./lib/python3.9/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205073b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620c6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/06 01:01:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark Introduction\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3cfbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://niyants-mbp.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Introduction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdbd1f169a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5b356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077a2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create emp DataFrame\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b51ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of partitions\n",
    "\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2fea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data (ACTION)\n",
    "\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf77ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our first Transformation (EMP salary > 50000)\n",
    "\n",
    "emp_final = emp.where(\"salary > 50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8c2ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate number of Partitions\n",
    "\n",
    "emp_final.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42747c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as CSV output (ACTION)\n",
    "\n",
    "emp_final.write.format(\"csv\").save(\"data/output/1/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d617ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c165318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('employee_id', StringType(), True), StructField('department_id', StringType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('salary', StringType(), True), StructField('hire_date', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for emp\n",
    "\n",
    "emp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91065a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema\n",
    "\n",
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e934de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Example for Schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema_string = \"name string, age int\"\n",
    "\n",
    "schema_spark =  StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9537c0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'salary'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns and expression\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "emp[\"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f917a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT columns\n",
    "# select employee_id, name, age, salary from emp\n",
    "\n",
    "emp_filtered = emp.select(col(\"employee_id\"), expr(\"name\"), emp.age, emp.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72886f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+\n",
      "|employee_id|         name|age|salary|\n",
      "+-----------+-------------+---+------+\n",
      "|        001|     John Doe| 30| 50000|\n",
      "|        002|   Jane Smith| 25| 45000|\n",
      "|        003|    Bob Brown| 35| 55000|\n",
      "|        004|    Alice Lee| 28| 48000|\n",
      "|        005|    Jack Chan| 40| 60000|\n",
      "|        006|    Jill Wong| 32| 52000|\n",
      "|        007|James Johnson| 42| 70000|\n",
      "|        008|     Kate Kim| 29| 51000|\n",
      "|        009|      Tom Tan| 33| 58000|\n",
      "|        010|     Lisa Lee| 27| 47000|\n",
      "|        011|   David Park| 38| 65000|\n",
      "|        012|   Susan Chen| 31| 54000|\n",
      "|        013|    Brian Kim| 45| 75000|\n",
      "|        014|    Emily Lee| 26| 46000|\n",
      "|        015|  Michael Lee| 37| 63000|\n",
      "|        016|  Kelly Zhang| 30| 49000|\n",
      "|        017|  George Wang| 34| 57000|\n",
      "|        018|    Nancy Liu| 29| 50000|\n",
      "|        019|  Steven Chen| 36| 62000|\n",
      "|        020|    Grace Kim| 32| 53000|\n",
      "+-----------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97015f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using expr for select\n",
    "# select employee_id as emp_id, name, cast(age as int) as age, salary from emp_filtered\n",
    "\n",
    "emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp.name, expr(\"cast(age as int) as age\"), emp.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0034b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   001|     John Doe| 30| 50000|\n",
      "|   002|   Jane Smith| 25| 45000|\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   004|    Alice Lee| 28| 48000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   008|     Kate Kim| 29| 51000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   010|     Lisa Lee| 27| 47000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   014|    Emily Lee| 26| 46000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   016|  Kelly Zhang| 30| 49000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   018|    Nancy Liu| 29| 50000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_casted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aff4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_casted_1 = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e375be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   001|     John Doe| 30| 50000|\n",
      "|   002|   Jane Smith| 25| 45000|\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   004|    Alice Lee| 28| 48000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   008|     Kate Kim| 29| 51000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   010|     Lisa Lee| 27| 47000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   014|    Emily Lee| 26| 46000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   016|  Kelly Zhang| 30| 49000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   018|    Nancy Liu| 29| 50000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d098af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fd974ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter emp based on Age > 30\n",
    "# select emp_id, name, age, salary from emp_casted where age > 30\n",
    "\n",
    "emp_final = emp_casted.select(\"emp_id\", \"name\", \"age\", \"salary\").where(\"age > 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e702e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a10103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data back as CSV (ACTION)\n",
    "\n",
    "emp_final.write.format(\"csv\").save(\"data/output/2/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28bc2e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Bonus TIP\n",
    "\n",
    "schema_str = \"name string, age int\"\n",
    "\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "\n",
    "schema_spark = _parse_datatype_string(schema_str)\n",
    "\n",
    "schema_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc7f72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting Column\n",
    "# select employee_id, name, age, cast(salary as double) as salary from emp\n",
    "from pyspark.sql.functions import col, cast\n",
    "\n",
    "emp_casted = emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a644a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dc2bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Columns\n",
    "# select employee_id, name, age, salary, (salary * 0.2) as tax from emp_casted\n",
    "\n",
    "emp_taxed = emp_casted.withColumn(\"tax\", col(\"salary\") * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f288a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|        001|     John Doe| 30|50000.0|10000.0|\n",
      "|        002|   Jane Smith| 25|45000.0| 9000.0|\n",
      "|        003|    Bob Brown| 35|55000.0|11000.0|\n",
      "|        004|    Alice Lee| 28|48000.0| 9600.0|\n",
      "|        005|    Jack Chan| 40|60000.0|12000.0|\n",
      "|        006|    Jill Wong| 32|52000.0|10400.0|\n",
      "|        007|James Johnson| 42|70000.0|14000.0|\n",
      "|        008|     Kate Kim| 29|51000.0|10200.0|\n",
      "|        009|      Tom Tan| 33|58000.0|11600.0|\n",
      "|        010|     Lisa Lee| 27|47000.0| 9400.0|\n",
      "|        011|   David Park| 38|65000.0|13000.0|\n",
      "|        012|   Susan Chen| 31|54000.0|10800.0|\n",
      "|        013|    Brian Kim| 45|75000.0|15000.0|\n",
      "|        014|    Emily Lee| 26|46000.0| 9200.0|\n",
      "|        015|  Michael Lee| 37|63000.0|12600.0|\n",
      "|        016|  Kelly Zhang| 30|49000.0| 9800.0|\n",
      "|        017|  George Wang| 34|57000.0|11400.0|\n",
      "|        018|    Nancy Liu| 29|50000.0|10000.0|\n",
      "|        019|  Steven Chen| 36|62000.0|12400.0|\n",
      "|        020|    Grace Kim| 32|53000.0|10600.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_taxed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bfc323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Literals\n",
    "# select employee_id, name, age, salary, tax, 1 as columnOne, 'two' as columnTwo from emp_taxed\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "emp_new_cols = emp_taxed.withColumn(\"columnOne\", lit(1)).withColumn(\"columnTwo\", lit('two'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a86313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|        001|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|        002|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|        003|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|        004|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|        005|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|        006|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|        007|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|        008|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|        009|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|        010|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|        011|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|        012|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|        013|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|        014|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|        015|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|        016|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|        017|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|        018|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|        019|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|        020|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_new_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ec61621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "# select employee_id as emp_id, name, age, salary, tax, columnOne, columnTwo from emp_new_cols\n",
    "\n",
    "emp_1 = emp_new_cols.withColumnRenamed(\"employee_id\", \"emp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42433d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|emp_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|   001|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|   002|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|   003|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|   004|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|   005|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|   006|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|   007|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|   008|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|   009|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|   010|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|   011|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|   012|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|   013|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|   014|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|   015|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|   016|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|   017|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|   018|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|   019|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|   020|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "413ac0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names with Spaces\n",
    "# select employee_id as emp_id, name, age, salary, tax, columnOne, columnTwo as `Column Two` from emp_new_cols\n",
    "\n",
    "emp_2 = emp_new_cols.withColumnRenamed(\"columnTwo\", \"Column Two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a8df4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+----------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|Column Two|\n",
      "+-----------+-------------+---+-------+-------+---------+----------+\n",
      "|        001|     John Doe| 30|50000.0|10000.0|        1|       two|\n",
      "|        002|   Jane Smith| 25|45000.0| 9000.0|        1|       two|\n",
      "|        003|    Bob Brown| 35|55000.0|11000.0|        1|       two|\n",
      "|        004|    Alice Lee| 28|48000.0| 9600.0|        1|       two|\n",
      "|        005|    Jack Chan| 40|60000.0|12000.0|        1|       two|\n",
      "|        006|    Jill Wong| 32|52000.0|10400.0|        1|       two|\n",
      "|        007|James Johnson| 42|70000.0|14000.0|        1|       two|\n",
      "|        008|     Kate Kim| 29|51000.0|10200.0|        1|       two|\n",
      "|        009|      Tom Tan| 33|58000.0|11600.0|        1|       two|\n",
      "|        010|     Lisa Lee| 27|47000.0| 9400.0|        1|       two|\n",
      "|        011|   David Park| 38|65000.0|13000.0|        1|       two|\n",
      "|        012|   Susan Chen| 31|54000.0|10800.0|        1|       two|\n",
      "|        013|    Brian Kim| 45|75000.0|15000.0|        1|       two|\n",
      "|        014|    Emily Lee| 26|46000.0| 9200.0|        1|       two|\n",
      "|        015|  Michael Lee| 37|63000.0|12600.0|        1|       two|\n",
      "|        016|  Kelly Zhang| 30|49000.0| 9800.0|        1|       two|\n",
      "|        017|  George Wang| 34|57000.0|11400.0|        1|       two|\n",
      "|        018|    Nancy Liu| 29|50000.0|10000.0|        1|       two|\n",
      "|        019|  Steven Chen| 36|62000.0|12400.0|        1|       two|\n",
      "|        020|    Grace Kim| 32|53000.0|10600.0|        1|       two|\n",
      "+-----------+-------------+---+-------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81252695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Column\n",
    "\n",
    "emp_dropped = emp_new_cols.drop(\"columnTwo\", \"columnOne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48334a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|        001|     John Doe| 30|50000.0|10000.0|\n",
      "|        002|   Jane Smith| 25|45000.0| 9000.0|\n",
      "|        003|    Bob Brown| 35|55000.0|11000.0|\n",
      "|        004|    Alice Lee| 28|48000.0| 9600.0|\n",
      "|        005|    Jack Chan| 40|60000.0|12000.0|\n",
      "|        006|    Jill Wong| 32|52000.0|10400.0|\n",
      "|        007|James Johnson| 42|70000.0|14000.0|\n",
      "|        008|     Kate Kim| 29|51000.0|10200.0|\n",
      "|        009|      Tom Tan| 33|58000.0|11600.0|\n",
      "|        010|     Lisa Lee| 27|47000.0| 9400.0|\n",
      "|        011|   David Park| 38|65000.0|13000.0|\n",
      "|        012|   Susan Chen| 31|54000.0|10800.0|\n",
      "|        013|    Brian Kim| 45|75000.0|15000.0|\n",
      "|        014|    Emily Lee| 26|46000.0| 9200.0|\n",
      "|        015|  Michael Lee| 37|63000.0|12600.0|\n",
      "|        016|  Kelly Zhang| 30|49000.0| 9800.0|\n",
      "|        017|  George Wang| 34|57000.0|11400.0|\n",
      "|        018|    Nancy Liu| 29|50000.0|10000.0|\n",
      "|        019|  Steven Chen| 36|62000.0|12400.0|\n",
      "|        020|    Grace Kim| 32|53000.0|10600.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec514a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data \n",
    "# select employee_id as emp_id, name, age, salary, tax, columnOne from emp_col_dropped where tax > 1000\n",
    "\n",
    "emp_filtered = emp_dropped.where(\"tax > 10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "299fa0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|        003|    Bob Brown| 35|55000.0|11000.0|\n",
      "|        005|    Jack Chan| 40|60000.0|12000.0|\n",
      "|        006|    Jill Wong| 32|52000.0|10400.0|\n",
      "|        007|James Johnson| 42|70000.0|14000.0|\n",
      "|        008|     Kate Kim| 29|51000.0|10200.0|\n",
      "|        009|      Tom Tan| 33|58000.0|11600.0|\n",
      "|        011|   David Park| 38|65000.0|13000.0|\n",
      "|        012|   Susan Chen| 31|54000.0|10800.0|\n",
      "|        013|    Brian Kim| 45|75000.0|15000.0|\n",
      "|        015|  Michael Lee| 37|63000.0|12600.0|\n",
      "|        017|  George Wang| 34|57000.0|11400.0|\n",
      "|        019|  Steven Chen| 36|62000.0|12400.0|\n",
      "|        020|    Grace Kim| 32|53000.0|10600.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f56c385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LIMIT data\n",
    "# select employee_id as emp_id, name, age, salary, tax, columnOne from emp_filtered limit 5\n",
    "\n",
    "emp_limit = emp_filtered.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a72f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---+-------+-------+\n",
      "|employee_id|     name|age| salary|    tax|\n",
      "+-----------+---------+---+-------+-------+\n",
      "|        003|Bob Brown| 35|55000.0|11000.0|\n",
      "|        005|Jack Chan| 40|60000.0|12000.0|\n",
      "+-----------+---------+---+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data\n",
    "\n",
    "emp_limit.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc9423b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bonus TIP\n",
    "# Add multiple columns\n",
    "\n",
    "columns = {\n",
    "    \"tax\" : col(\"salary\") * 0.2 ,\n",
    "    \"oneNumber\" : lit(1), \n",
    "    \"columnTwo\" : lit(\"two\")\n",
    "}\n",
    "\n",
    "emp_final = emp.withColumns(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3233849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------+---------+---------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|    tax|oneNumber|columnTwo|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------+---------+---------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|10000.0|        1|      two|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15| 9000.0|        1|      two|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|11000.0|        1|      two|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30| 9600.0|        1|      two|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|12000.0|        1|      two|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|10400.0|        1|      two|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|14000.0|        1|      two|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|10200.0|        1|      two|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|11600.0|        1|      two|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01| 9400.0|        1|      two|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|13000.0|        1|      two|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|10800.0|        1|      two|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|15000.0|        1|      two|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01| 9200.0|        1|      two|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|12600.0|        1|      two|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01| 9800.0|        1|      two|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|11400.0|        1|      two|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|10000.0|        1|      two|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|12400.0|        1|      two|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|10600.0|        1|      two|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c48ba7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b57db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case When\n",
    "# select employee_id, name, age, salary, gender,\n",
    "# case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end as new_gender, hire_date from emp\n",
    "from pyspark.sql.functions import when, col, expr\n",
    "\n",
    "emp_gender_fixed = emp.withColumn(\"new_gender\", when(col(\"gender\") == 'Male', 'M')\n",
    "                                 .when(col(\"gender\") == 'Female', 'F')\n",
    "                                 .otherwise(None)\n",
    "                                 )\n",
    "                            \n",
    "emp_gender_fixed_1 = emp.withColumn(\"new_gender\", expr(\"case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e583fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         F|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_gender_fixed_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1d38dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace in Strings\n",
    "# select employee_id, name, replace(name, 'J', 'Z') as new_name, age, salary, gender, new_gender, hire_date from emp_gender_fixed\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "emp_name_fixed = emp_gender_fixed.withColumn(\"new_name\", regexp_replace(col(\"name\"), \"J\", \"Z\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93f5e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|     new_name|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|     Zohn Doe|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|   Zane Smith|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|    Bob Brown|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|    Alice Lee|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|    Zack Chan|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|    Zill Wong|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|Zames Zohnson|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|     Kate Kim|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|      Tom Tan|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|     Lisa Lee|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|   David Park|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|   Susan Chen|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|    Brian Kim|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|    Emily Lee|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|  Michael Lee|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|  Kelly Zhang|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|  George Wang|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         F|    Nancy Liu|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|  Steven Chen|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|    Grace Kim|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_name_fixed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02468db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date\n",
    "# select *,  to_date(hire_date, 'YYYY-MM-DD') as hire_date from emp_name_fixed\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "emp_date_fix = emp_name_fixed.withColumn(\"hire_date\", to_date(col(\"hire_date\"), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a537d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- new_gender: string (nullable = true)\n",
      " |-- new_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_date_fix.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a85f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Date Columns\n",
    "# Add current_date, current_timestamp, extract year from hire_date\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "emp_dated = emp_date_fix.withColumn(\"date_now\", current_date()).withColumn(\"timestamp_now\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ab3c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date |new_gender|new_name     |date_now  |timestamp_now             |\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------------+\n",
      "|001        |101          |John Doe     |30 |Male  |50000 |2015-01-01|M         |Zohn Doe     |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|002        |101          |Jane Smith   |25 |Female|45000 |2016-02-15|F         |Zane Smith   |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|003        |102          |Bob Brown    |35 |Male  |55000 |2014-05-01|M         |Bob Brown    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|004        |102          |Alice Lee    |28 |Female|48000 |2017-09-30|F         |Alice Lee    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|005        |103          |Jack Chan    |40 |Male  |60000 |2013-04-01|M         |Zack Chan    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|006        |103          |Jill Wong    |32 |Female|52000 |2018-07-01|F         |Zill Wong    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|007        |101          |James Johnson|42 |Male  |70000 |2012-03-15|M         |Zames Zohnson|2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|008        |102          |Kate Kim     |29 |Female|51000 |2019-10-01|F         |Kate Kim     |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|009        |103          |Tom Tan      |33 |Male  |58000 |2016-06-01|M         |Tom Tan      |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|010        |104          |Lisa Lee     |27 |Female|47000 |2018-08-01|F         |Lisa Lee     |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|011        |104          |David Park   |38 |Male  |65000 |2015-11-01|M         |David Park   |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|012        |105          |Susan Chen   |31 |Female|54000 |2017-02-15|F         |Susan Chen   |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|013        |106          |Brian Kim    |45 |Male  |75000 |2011-07-01|M         |Brian Kim    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|014        |107          |Emily Lee    |26 |Female|46000 |2019-01-01|F         |Emily Lee    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|015        |106          |Michael Lee  |37 |Male  |63000 |2014-09-30|M         |Michael Lee  |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|016        |107          |Kelly Zhang  |30 |Female|49000 |2018-04-01|F         |Kelly Zhang  |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|017        |105          |George Wang  |34 |Male  |57000 |2016-03-15|M         |George Wang  |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|018        |104          |Nancy Liu    |29 |Female|50000 |2017-06-01|F         |Nancy Liu    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|019        |103          |Steven Chen  |36 |Male  |62000 |2015-08-01|M         |Steven Chen  |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "|020        |102          |Grace Kim    |32 |Female|53000 |2018-11-01|F         |Grace Kim    |2025-04-06|2025-04-06 01:01:58.198278|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dated.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09329d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null gender records\n",
    "emp_1 = emp_dated.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d408764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|     new_name|  date_now|       timestamp_now|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|     Zohn Doe|2025-04-06|2025-04-06 01:01:...|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|   Zane Smith|2025-04-06|2025-04-06 01:01:...|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|    Bob Brown|2025-04-06|2025-04-06 01:01:...|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|    Alice Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|    Zack Chan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|    Zill Wong|2025-04-06|2025-04-06 01:01:...|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|Zames Zohnson|2025-04-06|2025-04-06 01:01:...|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|     Kate Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|      Tom Tan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|     Lisa Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|   David Park|2025-04-06|2025-04-06 01:01:...|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|   Susan Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|    Brian Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|    Emily Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|  Michael Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|  Kelly Zhang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|  George Wang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         F|    Nancy Liu|2025-04-06|2025-04-06 01:01:...|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|  Steven Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|    Grace Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7876c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Null values\n",
    "# select *, nvl('new_gender', 'O') as new_gender from emp_dated\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "emp_null_df = emp_dated.withColumn(\"new_gender\", coalesce(col(\"new_gender\"), lit(\"O\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "831f98f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|     new_name|  date_now|       timestamp_now|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|     Zohn Doe|2025-04-06|2025-04-06 01:01:...|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|   Zane Smith|2025-04-06|2025-04-06 01:01:...|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|    Bob Brown|2025-04-06|2025-04-06 01:01:...|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|    Alice Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|    Zack Chan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|    Zill Wong|2025-04-06|2025-04-06 01:01:...|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|Zames Zohnson|2025-04-06|2025-04-06 01:01:...|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|     Kate Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|      Tom Tan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|     Lisa Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|   David Park|2025-04-06|2025-04-06 01:01:...|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|   Susan Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|    Brian Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|    Emily Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|  Michael Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|  Kelly Zhang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|  George Wang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         F|    Nancy Liu|2025-04-06|2025-04-06 01:01:...|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|  Steven Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|    Grace Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd9ce6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old columns and Fix new column names\n",
    "emp_final = emp_null_df.drop(\"name\", \"gender\").withColumnRenamed(\"new_name\", \"name\").withColumnRenamed(\"new_gender\", \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b4c85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+\n",
      "|employee_id|department_id|age|salary| hire_date|gender|         name|  date_now|       timestamp_now|\n",
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+\n",
      "|        001|          101| 30| 50000|2015-01-01|     M|     Zohn Doe|2025-04-06|2025-04-06 01:01:...|\n",
      "|        002|          101| 25| 45000|2016-02-15|     F|   Zane Smith|2025-04-06|2025-04-06 01:01:...|\n",
      "|        003|          102| 35| 55000|2014-05-01|     M|    Bob Brown|2025-04-06|2025-04-06 01:01:...|\n",
      "|        004|          102| 28| 48000|2017-09-30|     F|    Alice Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        005|          103| 40| 60000|2013-04-01|     M|    Zack Chan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        006|          103| 32| 52000|2018-07-01|     F|    Zill Wong|2025-04-06|2025-04-06 01:01:...|\n",
      "|        007|          101| 42| 70000|2012-03-15|     M|Zames Zohnson|2025-04-06|2025-04-06 01:01:...|\n",
      "|        008|          102| 29| 51000|2019-10-01|     F|     Kate Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        009|          103| 33| 58000|2016-06-01|     M|      Tom Tan|2025-04-06|2025-04-06 01:01:...|\n",
      "|        010|          104| 27| 47000|2018-08-01|     F|     Lisa Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        011|          104| 38| 65000|2015-11-01|     M|   David Park|2025-04-06|2025-04-06 01:01:...|\n",
      "|        012|          105| 31| 54000|2017-02-15|     F|   Susan Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        013|          106| 45| 75000|2011-07-01|     M|    Brian Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "|        014|          107| 26| 46000|2019-01-01|     F|    Emily Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        015|          106| 37| 63000|2014-09-30|     M|  Michael Lee|2025-04-06|2025-04-06 01:01:...|\n",
      "|        016|          107| 30| 49000|2018-04-01|     F|  Kelly Zhang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        017|          105| 34| 57000|2016-03-15|     M|  George Wang|2025-04-06|2025-04-06 01:01:...|\n",
      "|        018|          104| 29| 50000|2017-06-01|     F|    Nancy Liu|2025-04-06|2025-04-06 01:01:...|\n",
      "|        019|          103| 36| 62000|2015-08-01|     M|  Steven Chen|2025-04-06|2025-04-06 01:01:...|\n",
      "|        020|          102| 32| 53000|2018-11-01|     F|    Grace Kim|2025-04-06|2025-04-06 01:01:...|\n",
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9ced0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as CSV\n",
    "emp_final.write.format(\"csv\").save(\"data/output/4/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "556fec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus TIP\n",
    "# Convert date into String and extract date information\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "emp_fixed = emp_final.withColumn(\"date_year\", date_format(col(\"timestamp_now\"), \"z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ceadb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+---------+\n",
      "|employee_id|department_id|age|salary| hire_date|gender|         name|  date_now|       timestamp_now|date_year|\n",
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+---------+\n",
      "|        001|          101| 30| 50000|2015-01-01|     M|     Zohn Doe|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        002|          101| 25| 45000|2016-02-15|     F|   Zane Smith|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        003|          102| 35| 55000|2014-05-01|     M|    Bob Brown|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        004|          102| 28| 48000|2017-09-30|     F|    Alice Lee|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        005|          103| 40| 60000|2013-04-01|     M|    Zack Chan|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        006|          103| 32| 52000|2018-07-01|     F|    Zill Wong|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        007|          101| 42| 70000|2012-03-15|     M|Zames Zohnson|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        008|          102| 29| 51000|2019-10-01|     F|     Kate Kim|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        009|          103| 33| 58000|2016-06-01|     M|      Tom Tan|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        010|          104| 27| 47000|2018-08-01|     F|     Lisa Lee|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        011|          104| 38| 65000|2015-11-01|     M|   David Park|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        012|          105| 31| 54000|2017-02-15|     F|   Susan Chen|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        013|          106| 45| 75000|2011-07-01|     M|    Brian Kim|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        014|          107| 26| 46000|2019-01-01|     F|    Emily Lee|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        015|          106| 37| 63000|2014-09-30|     M|  Michael Lee|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        016|          107| 30| 49000|2018-04-01|     F|  Kelly Zhang|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        017|          105| 34| 57000|2016-03-15|     M|  George Wang|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        018|          104| 29| 50000|2017-06-01|     F|    Nancy Liu|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        019|          103| 36| 62000|2015-08-01|     M|  Steven Chen|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "|        020|          102| 32| 53000|2018-11-01|     F|    Grace Kim|2025-04-06|2025-04-06 01:01:...|     CEST|\n",
      "+-----------+-------------+---+------+----------+------+-------------+----------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_fixed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfdf8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort, union and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "272ca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data_1 = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"]\n",
    "]\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9f4ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create emp DataFrame\n",
    "\n",
    "emp_data_1 = spark.createDataFrame(data=emp_data_1, schema=emp_schema)\n",
    "emp_data_2 = spark.createDataFrame(data=emp_data_2, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a097fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|        011|          104| David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|  Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|  Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show emp dataframe (ACTION)\n",
    "\n",
    "emp_data_1.show()\n",
    "emp_data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6268fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema\n",
    "\n",
    "emp_data_1.printSchema()\n",
    "emp_data_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80c0ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION and UNION ALL\n",
    "# select * from emp_data_1 UNION select * from emp_data_2\n",
    "emp_uni = emp_data_1.unionAll(emp_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41f39b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_uni.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee69cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the emp data based on desc Salary\n",
    "# select * from emp order by salary desc\n",
    "from pyspark.sql.functions import desc, asc, col\n",
    "\n",
    "emp_sorted = emp_uni.orderBy(col(\"salary\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2b71bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5d4cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation\n",
    "# select dept_id, count(employee_id) as total_dept_count from emp_sorted group by dept_id \n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "emp_count = emp_sorted.groupBy(\"department_id\").agg(count(\"employee_id\").alias(\"total_dept_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eef34953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|department_id|total_dept_count|\n",
      "+-------------+----------------+\n",
      "|          101|               3|\n",
      "|          102|               4|\n",
      "|          103|               4|\n",
      "|          104|               3|\n",
      "|          105|               2|\n",
      "|          106|               2|\n",
      "|          107|               2|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09b0a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation\n",
    "# select dept_id, sum(salary) as total_dept_salary from emp_sorted group by dept_id \n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "emp_sum = emp_sorted.groupBy(\"department_id\").agg(sum(\"salary\").alias(\"total_dept_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ebbf8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|department_id|total_dept_salary|\n",
      "+-------------+-----------------+\n",
      "|          101|         165000.0|\n",
      "|          107|          95000.0|\n",
      "|          104|         162000.0|\n",
      "|          102|         207000.0|\n",
      "|          103|         232000.0|\n",
      "|          106|         138000.0|\n",
      "|          105|         111000.0|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ae72a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation with having clause\n",
    "# select dept_id, avg(salary) as avg_dept_salary from emp_sorted  group by dept_id having avg(salary) > 50000\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp_sorted.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_dept_salary\")).where(\"avg_dept_salary > 50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce688191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|department_id|avg_dept_salary|\n",
      "+-------------+---------------+\n",
      "|          101|        55000.0|\n",
      "|          104|        54000.0|\n",
      "|          102|        51750.0|\n",
      "|          103|        58000.0|\n",
      "|          106|        69000.0|\n",
      "|          105|        55500.0|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9335900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bonus TIP - unionByName\n",
    "# In case the column sequence is different\n",
    "emp_data_2_other = emp_data_2.select(\"employee_id\", \"salary\", \"department_id\", \"name\", \"hire_date\", \"gender\", \"age\")\n",
    "\n",
    "emp_data_1.printSchema()\n",
    "emp_data_2_other.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a6658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_fixed = emp_data_1.unionByName(emp_data_2_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00d44590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_fixed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f122623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c844b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique data and Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "576d1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique data\n",
    "# select distinct emp.* from emp\n",
    "emp_unique = emp.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae3cbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique of department_ids\n",
    "# select distinct department_id from emp\n",
    "emp_dept_id = emp.select(\"department_id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca4376fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|department_id|\n",
      "+-------------+\n",
      "|          101|\n",
      "|          102|\n",
      "|          103|\n",
      "|          104|\n",
      "|          105|\n",
      "|          107|\n",
      "|          106|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54893fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Functions\n",
    "# select *, max(salary) over(partition by department_id order by salary desc) as max_salary from emp_unique\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, col, desc\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "max_func = max(col(\"salary\")).over(window_spec)\n",
    "\n",
    "emp_1 = emp.withColumn(\"max_salary\", max_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ef25a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|max_salary|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|     70000|\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     70000|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|     70000|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|     55000|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|     55000|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|     55000|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|     55000|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|     62000|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     62000|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|     62000|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|     62000|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|     65000|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|     65000|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|     65000|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|     57000|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|     57000|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     75000|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|     75000|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|     49000|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|     49000|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show()\n",
    "\n",
    "# Window Functions - 2nd highest salary of each department\n",
    "# select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "rn = row_number().over(window_spec)\n",
    "\n",
    "emp_2 = emp.withColumn(\"rn\", rn).where(\"rn = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2780f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date| rn|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|        001|          101|   John Doe| 30|  Male| 50000|2015-01-01|  2|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|  2|\n",
      "|        005|          103|  Jack Chan| 40|  Male| 60000|2013-04-01|  2|\n",
      "|        018|          104|  Nancy Liu| 29|Female| 50000|2017-06-01|  2|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|  2|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|  2|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|  2|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "47271b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window function using expr\n",
    "# select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "emp_3 = emp.withColumn(\"rn\", expr(\"row_number() over(partition by department_id order by salary desc)\")).where(\"rn = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68dc001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date| rn|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "|        001|          101|   John Doe| 30|  Male| 50000|2015-01-01|  2|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|  2|\n",
      "|        005|          103|  Jack Chan| 40|  Male| 60000|2013-04-01|  2|\n",
      "|        018|          104|  Nancy Liu| 29|Female| 50000|2017-06-01|  2|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|  2|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|  2|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|  2|\n",
      "+-----------+-------------+-----------+---+------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf2fd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus TIP\n",
    "# Databricks community cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b877085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins and data partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9dd806a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_data = [\n",
    "    [\"101\", \"Sales\", \"NYC\", \"US\", \"1000000\"],\n",
    "    [\"102\", \"Marketing\", \"LA\", \"US\", \"900000\"],\n",
    "    [\"103\", \"Finance\", \"London\", \"UK\", \"1200000\"],\n",
    "    [\"104\", \"Engineering\", \"Beijing\", \"China\", \"1500000\"],\n",
    "    [\"105\", \"Human Resources\", \"Tokyo\", \"Japan\", \"800000\"],\n",
    "    [\"106\", \"Research and Development\", \"Perth\", \"Australia\", \"1100000\"],\n",
    "    [\"107\", \"Customer Service\", \"Sydney\", \"Australia\", \"950000\"]\n",
    "]\n",
    "\n",
    "dept_schema = \"department_id string, department_name string, city string, country string, budget string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "45ca00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dept DataFrame\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db4809e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|department_id|     department_name|   city|  country| budget|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|          101|               Sales|    NYC|       US|1000000|\n",
      "|          102|           Marketing|     LA|       US| 900000|\n",
      "|          103|             Finance| London|       UK|1200000|\n",
      "|          104|         Engineering|Beijing|    China|1500000|\n",
      "|          105|     Human Resources|  Tokyo|    Japan| 800000|\n",
      "|          106|Research and Deve...|  Perth|Australia|1100000|\n",
      "|          107|    Customer Service| Sydney|Australia| 950000|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show emp dataframe (ACTION)\n",
    "\n",
    "emp.show()\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9afc33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema\n",
    "\n",
    "emp.printSchema()\n",
    "dept.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14a426f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions for emp\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da22fdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions for dept\n",
    "dept.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eecf9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition of data using repartition & coalesce\n",
    "emp_partitioned = emp.repartition(4, \"department_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5563dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_partitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2967cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the partition info for partitions and reparition\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp_1 = emp.repartition(4, \"department_id\").withColumn(\"partition_num\", spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e3b81621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_num|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|            0|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|            0|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|            0|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|            0|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            0|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|            0|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|            2|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|            2|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|            2|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|            2|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|            2|\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|            3|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|            3|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|            3|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|            3|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|            3|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|            3|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|            3|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "451aa817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN datasets\n",
    "# select e.emp_name, d.department_name, d.department_id, e.salary \n",
    "# from emp e inner join dept d on emp.department_id = dept.department_id\n",
    "\n",
    "df_joined = emp.alias(\"e\").join(dept.alias(\"d\"), how=\"inner\", on=emp.department_id==dept.department_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2704dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------------+------+\n",
      "|         name|department_id|     department_name|salary|\n",
      "+-------------+-------------+--------------------+------+\n",
      "|     John Doe|          101|               Sales| 50000|\n",
      "|   Jane Smith|          101|               Sales| 45000|\n",
      "|James Johnson|          101|               Sales| 70000|\n",
      "|    Bob Brown|          102|           Marketing| 55000|\n",
      "|    Alice Lee|          102|           Marketing| 48000|\n",
      "|     Kate Kim|          102|           Marketing| 51000|\n",
      "|    Grace Kim|          102|           Marketing| 53000|\n",
      "|    Jack Chan|          103|             Finance| 60000|\n",
      "|    Jill Wong|          103|             Finance| 52000|\n",
      "|      Tom Tan|          103|             Finance| 58000|\n",
      "|  Steven Chen|          103|             Finance| 62000|\n",
      "|     Lisa Lee|          104|         Engineering| 47000|\n",
      "|   David Park|          104|         Engineering| 65000|\n",
      "|    Nancy Liu|          104|         Engineering| 50000|\n",
      "|   Susan Chen|          105|     Human Resources| 54000|\n",
      "|  George Wang|          105|     Human Resources| 57000|\n",
      "|    Brian Kim|          106|Research and Deve...| 75000|\n",
      "|  Michael Lee|          106|Research and Deve...| 63000|\n",
      "|    Emily Lee|          107|    Customer Service| 46000|\n",
      "|  Kelly Zhang|          107|    Customer Service| 49000|\n",
      "+-------------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\"e.name\", \"d.department_id\", \"d.department_name\", \"e.salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0ce7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT OUTER JOIN datasets\n",
    "# select e.emp_name, d.department_name, d.department_id, e.salary \n",
    "# from emp e left outer join dept d on emp.department_id = dept.department_id\n",
    "\n",
    "df_joined = emp.alias(\"e\").join(dept.alias(\"d\"), how=\"left_outer\", on=emp.department_id==dept.department_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "540dfe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------------+------+\n",
      "|         name|     department_name|department_id|salary|\n",
      "+-------------+--------------------+-------------+------+\n",
      "|     John Doe|               Sales|          101| 50000|\n",
      "|   Jane Smith|               Sales|          101| 45000|\n",
      "|    Bob Brown|           Marketing|          102| 55000|\n",
      "|    Alice Lee|           Marketing|          102| 48000|\n",
      "|    Jack Chan|             Finance|          103| 60000|\n",
      "|    Jill Wong|             Finance|          103| 52000|\n",
      "|James Johnson|               Sales|          101| 70000|\n",
      "|     Lisa Lee|         Engineering|          104| 47000|\n",
      "|     Kate Kim|           Marketing|          102| 51000|\n",
      "|      Tom Tan|             Finance|          103| 58000|\n",
      "|   David Park|         Engineering|          104| 65000|\n",
      "|   Susan Chen|     Human Resources|          105| 54000|\n",
      "|    Emily Lee|    Customer Service|          107| 46000|\n",
      "|    Brian Kim|Research and Deve...|          106| 75000|\n",
      "|  Kelly Zhang|    Customer Service|          107| 49000|\n",
      "|  Michael Lee|Research and Deve...|          106| 63000|\n",
      "|    Nancy Liu|         Engineering|          104| 50000|\n",
      "|    Grace Kim|           Marketing|          102| 53000|\n",
      "|  Steven Chen|             Finance|          103| 62000|\n",
      "|  George Wang|     Human Resources|          105| 57000|\n",
      "+-------------+--------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\"e.name\", \"d.department_name\", \"d.department_id\", \"e.salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af9b542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final dataset\n",
    "df_joined.select(\"e.name\", \"d.department_name\", \"d.department_id\",\"e.salary\").write.format(\"csv\").save(\"data/output/7/emp_joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f4e5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus TIP\n",
    "# Joins with cascading conditions\n",
    "# Join with Department_id and only for departments 101 or 102\n",
    "# Join with not null/null conditions\n",
    "\n",
    "df_final = emp.join(dept, how=\"left_outer\", \n",
    "                   on=(emp.department_id==dept.department_id) & ((emp.department_id == \"101\") | (emp.department_id == \"102\")) \n",
    "                    & (emp.salary.isNull())\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c21d81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+---------------+----+-------+------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|department_id|department_name|city|country|budget|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+---------------+----+-------+------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         NULL|           NULL|NULL|   NULL|  NULL|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+---------------+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f3f7732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fccfb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file into dataframe\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"data/input/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d8bde844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "58920e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb3b4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading with Schema\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "\n",
    "df_schema = spark.read.format(\"csv\").option(\"header\",True).schema(_schema).load(\"data/input/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f818501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
      "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n",
      "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
      "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|\n",
      "|          7|          101|James Johnson| 42|  Male|70000.0|2012-03-15|\n",
      "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|\n",
      "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
      "|         11|          104|   David Park| 38|  Male|65000.0|2015-11-01|\n",
      "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|\n",
      "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
      "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|\n",
      "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
      "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
      "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|\n",
      "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
      "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0bcc2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle BAD records - PERMISSIVE (Default mode)\n",
    "\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, bad_record string\"\n",
    "\n",
    "df_p = spark.read.format(\"csv\").schema(_schema).option(\"columnNameOfCorruptRecord\", \"bad_record\").option(\"header\", True).load(\"data/input/emp_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95447b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- bad_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fe68c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender| salary| hire_date|          bad_record|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|                NULL|\n",
      "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|                NULL|\n",
      "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|                NULL|\n",
      "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|                NULL|\n",
      "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|                NULL|\n",
      "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|                NULL|\n",
      "|          7|          101|James Johnson| 42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
      "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|                NULL|\n",
      "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|                NULL|\n",
      "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|                NULL|\n",
      "|         11|          104|   David Park| 38|  Male|65000.0|      NULL|011,104,David Par...|\n",
      "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|                NULL|\n",
      "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|                NULL|\n",
      "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|                NULL|\n",
      "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|                NULL|\n",
      "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|                NULL|\n",
      "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|                NULL|\n",
      "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|                NULL|\n",
      "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|                NULL|\n",
      "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|                NULL|\n",
      "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "020d5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle BAD records - DROPMALFORMED\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "\n",
    "df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(_schema).load(\"data/input/emp_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "37361646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0c9d0bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+-------+----------+\n",
      "|employee_id|department_id|       name|age|gender| salary| hire_date|\n",
      "+-----------+-------------+-----------+---+------+-------+----------+\n",
      "|          1|          101|   John Doe| 30|  Male|50000.0|2015-01-01|\n",
      "|          2|          101| Jane Smith| 25|Female|45000.0|2016-02-15|\n",
      "|          3|          102|  Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
      "|          4|          102|  Alice Lee| 28|Female|48000.0|2017-09-30|\n",
      "|          5|          103|  Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
      "|          6|          103|  Jill Wong| 32|Female|52000.0|2018-07-01|\n",
      "|          8|          102|   Kate Kim| 29|Female|51000.0|2019-10-01|\n",
      "|          9|          103|    Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
      "|         10|          104|   Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
      "|         12|          105| Susan Chen| 31|Female|54000.0|2017-02-15|\n",
      "|         13|          106|  Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
      "|         14|          107|  Emily Lee| 26|Female|46000.0|2019-01-01|\n",
      "|         15|          106|Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
      "|         16|          107|Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
      "|         17|          105|George Wang| 34|  Male|57000.0|2016-03-15|\n",
      "|         18|          104|  Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
      "|         19|          103|Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
      "|         20|          102|  Grace Kim| 32|Female|53000.0|2018-11-01|\n",
      "+-----------+-------------+-----------+---+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6fceacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle BAD records - FAILFAST\n",
    "\n",
    "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
    "\n",
    "df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(_schema).load(\"data/input/emp_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "53cbf432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28e58705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 01:02:05 ERROR Executor: Exception in task 0.0 in stage 146.0 (TID 488)\n",
      "org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n",
      "\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n",
      "\t... 29 more\n",
      "25/04/06 01:02:05 WARN TaskSetManager: Lost task 0.0 in stage 146.0 (TID 488) (niyants-mbp.fritz.box executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Low\"\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
      "\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n",
      "\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n",
      "\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n",
      "\t... 29 more\n",
      "\n",
      "25/04/06 01:02:05 ERROR TaskSetManager: Task 0 in stage 146.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o416.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 146.0 failed 1 times, most recent failure: Lost task 0.0 in stage 146.0 (TID 488) (niyants-mbp.fritz.box executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o416.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 146.0 failed 1 times, most recent failure: Lost task 0.0 in stage 146.0 (TID 488) (niyants-mbp.fritz.box executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:208)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:204)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "df_m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "feda5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS TIP\n",
    "# Multiple options\n",
    "\n",
    "_options = {\n",
    "    \"header\" : \"true\",\n",
    "    \"inferSchema\" : \"true\",\n",
    "    \"mode\" : \"PERMISSIVE\"\n",
    "}\n",
    "\n",
    "df = (spark.read.format(\"csv\").options(**_options).load(\"data/input/emp.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8533c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a15527bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading_complex_data_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e69f6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet Sales data\n",
    "\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"data/input/sales_total_parquet/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8340159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a624d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-12-27 20:00:00| 330765426|  887300947|Kroger   ccd id: ...|  33.56|2068475652|\n",
      "|2017-11-26 22:00:00|1377679664| 1070485878|Amazon.com    ccd...|  24.43|1640819540|\n",
      "|2017-12-13 00:00:00| 472018705| 2001148981|  unkn      Columbia|   1.24| 481821583|\n",
      "|2017-05-19 21:00:00|1127671830|  847200066|            Wal-Mart|2155.48|2074005445|\n",
      "|2017-11-17 22:00:00| 233137169|  847200066|            Wal-Mart|   4.13|2043825401|\n",
      "|2017-12-15 13:00:00| 603124844|  887300947|Kroger   ccd id: ...|  31.92|1640819540|\n",
      "|2017-11-08 13:00:00|1591888712|  143327090|  Menard       11-08|   42.3|2043825401|\n",
      "|2017-12-23 13:00:00|1775468459|  887300947|Kroger  arc id: 1...|  284.8|2055198208|\n",
      "|2017-09-01 15:00:00|1020833609| 2120842315|Burger King   ccd...|   20.2|1141716004|\n",
      "|2017-11-30 13:00:00|1714628652| 1898522855|Target    ppd id:...| 488.42| 569532635|\n",
      "|2017-11-27 00:00:00|1533320464|   62988535|Bed Bath & Beyond...|    6.1|1098014353|\n",
      "|2017-11-28 22:00:00|2057742958|  562903918|McDonald's     pp...|2051.68|1487857621|\n",
      "|2017-01-25 20:00:00| 123604891|  847200066|Wal-Mart    ccd i...|  23.53|  45522086|\n",
      "|2017-05-21 23:00:00|1660120794|  865681996|Nordstrom     arc...| 708.81|  28424447|\n",
      "|2017-12-18 23:00:00|1882911667|  997626433|Sears        Anka...| 160.29|1698762556|\n",
      "|2017-11-24 21:00:00| 734317675|  511877722|Best Buy    arc i...| 1104.7|2001708947|\n",
      "|2017-12-21 19:00:00| 513313448|  887300947|unkn    arc id: 9...|   32.7|2023698313|\n",
      "|2017-12-13 20:00:00|1380128813|  582210968|unkn     ppd id: ...|  19.88|1359730291|\n",
      "|2017-11-15 21:00:00| 941860469|  887300947|Kroger     arc id...|  55.69|  31398417|\n",
      "|2017-12-07 00:00:00|1957163946|  400404203|unkn   arc id: 97...|   9.24|1284771958|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 151:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6c6e4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORC Sales data\n",
    "\n",
    "df_orc = spark.read.format(\"orc\").load(\"data/input/sales_total_orc/*.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3a5c4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9a6816b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-12-27 19:00:00| 330765426|  887300947|Kroger   ccd id: ...|  33.56|2068475652|\n",
      "|2017-11-26 21:00:00|1377679664| 1070485878|Amazon.com    ccd...|  24.43|1640819540|\n",
      "|2017-12-12 23:00:00| 472018705| 2001148981|  unkn      Columbia|   1.24| 481821583|\n",
      "|2017-05-19 19:00:00|1127671830|  847200066|            Wal-Mart|2155.48|2074005445|\n",
      "|2017-11-17 21:00:00| 233137169|  847200066|            Wal-Mart|   4.13|2043825401|\n",
      "|2017-12-15 12:00:00| 603124844|  887300947|Kroger   ccd id: ...|  31.92|1640819540|\n",
      "|2017-11-08 12:00:00|1591888712|  143327090|  Menard       11-08|   42.3|2043825401|\n",
      "|2017-12-23 12:00:00|1775468459|  887300947|Kroger  arc id: 1...|  284.8|2055198208|\n",
      "|2017-09-01 13:00:00|1020833609| 2120842315|Burger King   ccd...|   20.2|1141716004|\n",
      "|2017-11-30 12:00:00|1714628652| 1898522855|Target    ppd id:...| 488.42| 569532635|\n",
      "|2017-11-26 23:00:00|1533320464|   62988535|Bed Bath & Beyond...|    6.1|1098014353|\n",
      "|2017-11-28 21:00:00|2057742958|  562903918|McDonald's     pp...|2051.68|1487857621|\n",
      "|2017-01-25 19:00:00| 123604891|  847200066|Wal-Mart    ccd i...|  23.53|  45522086|\n",
      "|2017-05-21 21:00:00|1660120794|  865681996|Nordstrom     arc...| 708.81|  28424447|\n",
      "|2017-12-18 22:00:00|1882911667|  997626433|Sears        Anka...| 160.29|1698762556|\n",
      "|2017-11-24 20:00:00| 734317675|  511877722|Best Buy    arc i...| 1104.7|2001708947|\n",
      "|2017-12-21 18:00:00| 513313448|  887300947|unkn    arc id: 9...|   32.7|2023698313|\n",
      "|2017-12-13 19:00:00|1380128813|  582210968|unkn     ppd id: ...|  19.88|1359730291|\n",
      "|2017-11-15 20:00:00| 941860469|  887300947|Kroger     arc id...|  55.69|  31398417|\n",
      "|2017-12-06 23:00:00|1957163946|  400404203|unkn   arc id: 97...|   9.24|1284771958|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e19e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benefits of Columnar Storage\n",
    "\n",
    "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
    "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fe643dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 146.03686332702637 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    df = spark.read.format(\"parquet\").load(\"data/input/sales_data.parquet\")\n",
    "    df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0fafb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 106.89997673034668 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    df = spark.read.format(\"parquet\").load(\"data/input/sales_data.parquet\")\n",
    "    df.select(\"trx_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aff7987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsales_recursive\\n|__ sales_1\\x01.parquet\\n|__ sales_1\\\\sales_2\\x02.parquet\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS TIP\n",
    "# RECURSIVE READ\n",
    "\n",
    "'''\n",
    "sales_recursive\n",
    "|__ sales_1\\1.parquet\n",
    "|__ sales_1\\sales_2\\2.parquet\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "855d2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.read.format(\"parquet\").load(\"data/input/sales_recursive/sales_1/1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c18b68e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|2017-11-24 20:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ccb1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.format(\"parquet\").load(\"data/input/sales_recursive/sales_1/sales_2/2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9d75d632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+------+--------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description|amount| city_id|\n",
      "+-------------------+----------+-----------+--------------------+------+--------+\n",
      "|2017-11-24 20:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55|45522086|\n",
      "+-------------------+----------+-----------+--------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d5574438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", True).load(\"data/input/sales_recursive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "23950c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = (\n",
    "    spark.read.format(\"parquet\")\n",
    "    .option(\"recursiveFileLookup\", True)\n",
    "    .option(\"pathGlobFilter\", \"*.parquet\")  # Only read .parquet files\n",
    "    .load(\"data/input/sales_recursive/\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b33ce169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|2017-11-24 20:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55| 45522086|\n",
      "|2017-11-24 20:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bfacd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1b23287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Single line JSON file\n",
    "\n",
    "df_single = spark.read.format(\"json\").load(\"data/input/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "00f8a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f3903a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "493767e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Multiline JSON file\n",
    "\n",
    "df_multi = spark.read.format(\"json\").option(\"multiLine\", True).load(\"data/input/order_multiline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2f93aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f6a9c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "009df0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"text\").load(\"data/input/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f57c6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4e44f7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"order_id\":\"O101\",\"customer_id\":\"C001\",\"order_line_items\":[{\"item_id\":\"I001\",\"qty\":6,\"amount\":102.45},{\"item_id\":\"I003\",\"qty\":2,\"amount\":2.01}],\"contact\":[9000010000,9000010001]}|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fd638f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Schema\n",
    "\n",
    "_schema = \"customer_id string, order_id string, contact array<long>\"\n",
    "\n",
    "df_schema = spark.read.format(\"json\").schema(_schema).load(\"data/input/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "02282731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|customer_id|order_id|             contact|\n",
      "+-----------+--------+--------------------+\n",
      "|       C001|    O101|[9000010000, 9000...|\n",
      "+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "53659b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nroot\\n |-- contact: array (nullable = true)\\n |    |-- element: long (containsNull = true)\\n |-- customer_id: string (nullable = true)\\n |-- order_id: string (nullable = true)\\n |-- order_line_items: array (nullable = true)\\n |    |-- element: struct (containsNull = true)\\n |    |    |-- amount: double (nullable = true)\\n |    |    |-- item_id: string (nullable = true)\\n |    |    |-- qty: long (nullable = true)\\n'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "root\n",
    " |-- contact: array (nullable = true)\n",
    " |    |-- element: long (containsNull = true)\n",
    " |-- customer_id: string (nullable = true)\n",
    " |-- order_id: string (nullable = true)\n",
    " |-- order_line_items: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- amount: double (nullable = true)\n",
    " |    |    |-- item_id: string (nullable = true)\n",
    " |    |    |-- qty: long (nullable = true)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e0754f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f134c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema_new = spark.read.format(\"json\").schema(_schema).load(\"data/input/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "69a9e85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "534e85a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d461c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from_json to read from a column\n",
    "\n",
    "_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
    "\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df_expanded = df.withColumn(\"parsed\", from_json(df.value, _schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a3889f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- contact: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- order_id: string (nullable = true)\n",
      " |    |-- order_line_items: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- amount: double (nullable = true)\n",
      " |    |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_expanded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2912f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|              parsed|\n",
      "+--------------------+--------------------+\n",
      "|{\"order_id\":\"O101...|{[9000010000, 900...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_expanded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e0874138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to_json to parse a JSON string\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df_unparsed = df_expanded.withColumn(\"unparsed\", to_json(df_expanded.parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0aad01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- contact: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- order_id: string (nullable = true)\n",
      " |    |-- order_line_items: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- amount: double (nullable = true)\n",
      " |    |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |    |-- qty: long (nullable = true)\n",
      " |-- unparsed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unparsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0834b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|unparsed                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"contact\":[\"9000010000\",\"9000010001\"],\"customer_id\":\"C001\",\"order_id\":\"O101\",\"order_line_items\":[{\"amount\":102.45,\"item_id\":\"I001\",\"qty\":6},{\"amount\":2.01,\"item_id\":\"I003\",\"qty\":2}]}|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unparsed.select(\"unparsed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4dbfcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values from Parsed JSON\n",
    "\n",
    "df_1 = df_expanded.select(\"parsed.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "daff1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_2 = df_1.withColumn(\"expanded_line_items\", explode(\"order_line_items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d251a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+-------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|expanded_line_items|\n",
      "+--------------------+-----------+--------+--------------------+-------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|  {102.45, I001, 6}|\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|    {2.01, I003, 2}|\n",
      "+--------------------+-----------+--------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "004be149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.select(\"contact\", \"customer_id\", \"order_id\", \"expanded_line_items.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "70d80c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+------+-------+---+\n",
      "|             contact|customer_id|order_id|amount|item_id|qty|\n",
      "+--------------------+-----------+--------+------+-------+---+\n",
      "|[9000010000, 9000...|       C001|    O101|102.45|   I001|  6|\n",
      "|[9000010000, 9000...|       C001|    O101|  2.01|   I003|  2|\n",
      "+--------------------+-----------+--------+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ee9d0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode Array fields\n",
    "df_final = df_3.withColumn(\"contact_expanded\", explode(\"contact\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2a0a1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- contact_expanded: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "010a5b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+-------+---+----------------+\n",
      "|customer_id|order_id|amount|item_id|qty|contact_expanded|\n",
      "+-----------+--------+------+-------+---+----------------+\n",
      "|       C001|    O101|102.45|   I001|  6|      9000010000|\n",
      "|       C001|    O101|102.45|   I001|  6|      9000010001|\n",
      "|       C001|    O101|  2.01|   I003|  2|      9000010000|\n",
      "|       C001|    O101|  2.01|   I003|  2|      9000010001|\n",
      "+-----------+--------+------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.drop(\"contact\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b43bef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "eacf93a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark available cores with defaultParallism in Spark UI\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "968e350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 01:03:02 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Write the data in parquet format\n",
    "\n",
    "emp.write.format(\"parquet\").save(\"data/output/11/2/emp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0ec5d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           1|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           1|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           2|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           2|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           3|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           3|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           3|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           3|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           4|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           4|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           5|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           5|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           6|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           6|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           7|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|           7|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           7|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           7|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View data partition information\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8711d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.write.format(\"csv\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "22c7427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data with Partition to output location\n",
    "\n",
    "emp.write.format(\"csv\").partitionBy(\"department_id\").option(\"header\", True).save(\"data/output/11/4/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2df44b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/niyantmehta/Spiced/pyspark Project/myenv/data/output/11/3/emp.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write Modes - append, overwrite, ignore and error\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43memp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/output/11/3/emp.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/niyantmehta/Spiced/pyspark Project/myenv/data/output/11/3/emp.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Write Modes - append, overwrite, ignore and error\n",
    "\n",
    "emp.write.format(\"csv\").mode(\"error\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0bbed434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus TIP\n",
    "# What if we need to write only 1 output file to share with DownStream?\n",
    "\n",
    "emp.repartition(1).write.format(\"csv\").option(\"header\", True).save(\"data/output/11/5/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba9746fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Settion\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c4e460ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Cluster Execution\")\n",
    "    .master(\"spark://localhost:7077\")\n",
    "    .config(\"spark.executor.instances\", 2)\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4e28b3ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/sql/session.py:618\u001b[0m, in \u001b[0;36mSparkSession._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124m        <div>\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[38;5;124m</b></p>\u001b[39m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    617\u001b[0m         catalogImplementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalogImplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m--> 618\u001b[0m         sc_HTML\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr_html_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    619\u001b[0m     )\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/context.py:397\u001b[0m, in \u001b[0;36mSparkContext._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;43m    <div>\u001b[39;49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;43m        <p><b>SparkContext</b></p>\u001b[39;49m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;43m        <p><a href=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{sc.uiWebUrl}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>Spark UI</a></p>\u001b[39;49m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;43m        <dl>\u001b[39;49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;43m          <dt>Version</dt>\u001b[39;49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;43m            <dd><code>v\u001b[39;49m\u001b[38;5;132;43;01m{sc.version}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;43m          <dt>Master</dt>\u001b[39;49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.master}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;43m          <dt>AppName</dt>\u001b[39;49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.appName}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;43m        </dl>\u001b[39;49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;43m    </div>\u001b[39;49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Spiced/pyspark Project/myenv/lib/python3.9/site-packages/pyspark/context.py:603\u001b[0m, in \u001b[0;36mSparkContext.uiWebUrl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21muiWebUrl\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    589\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the URL of the SparkUI instance started by this :class:`SparkContext`\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.1.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m    'http://...'\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     jurl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39muiWebUrl()\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jurl\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mif\u001b[39;00m jurl\u001b[38;5;241m.\u001b[39mnonEmpty() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdbd1f169a0>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample data frame\n",
    "\n",
    "df = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623aad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data of the data frame\n",
    "\n",
    "df.write.format(\"csv\").option(\"header\", True).save(\"/data/output/15/3/range.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd98157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Settion\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edeae1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
